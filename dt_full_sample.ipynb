{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('fashion_train.npy').astype(np.int8)\n",
    "data_test = np.load('fashion_test.npy').astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree\n",
    "\n",
    "#Load data\n",
    "\n",
    "#Find split\n",
    "\n",
    "#   For every pair of point and feature, find a split with lowest impurity\n",
    "\n",
    "#   Store the split somewhere -- we don't have to store the\n",
    "\n",
    "#   New split, consider all the points in the sub-trees. Again, choose the split with the lowest impurity measure.\n",
    "\n",
    "#Choose a stopping condition: minimum information gain, maximum tree height, etc?\n",
    "\n",
    "#Store the set of decision rules somehow - we don't have to store it in the initial tree.\n",
    "#The tree can be built first and then the desicion rules can be written based on the thresholds and the class majority.\n",
    "\n",
    "#OR Pruning?\n",
    "\n",
    "#Interface for training and classifying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tree:\n",
    "    \n",
    "    class node:\n",
    "        def __init__(self,  left = None, right = None, feature = None, threshold = None, prediction = None):\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.prediction = prediction\n",
    "\n",
    "\n",
    "    def __init__(self, root= None):\n",
    "        self.root = root\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(label_counts, total_count):\n",
    "        if total_count == 0:\n",
    "            return 0\n",
    "        label_counts = np.array(label_counts)\n",
    "        proportions = label_counts / total_count\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_entropy(label_counts, total_count):\n",
    "        if total_count == 0:\n",
    "            return 0\n",
    "        label_counts = np.array(label_counts)\n",
    "        proportions = label_counts / total_count\n",
    "        # Avoid log(0) by masking zero proportions\n",
    "        valid_proportions = proportions[proportions > 0]\n",
    "        return - np.sum(valid_proportions * np.log(valid_proportions))\n",
    "\n",
    "    @staticmethod\n",
    "    def split_single_variable_optimized(data, impur = \"gini\"):\n",
    "        n = len(data)\n",
    "        \n",
    "        # Sort data by the feature (first column)\n",
    "        sorted_data = data[data[:, 0].argsort()]\n",
    "        \n",
    "        # Total bincount for the whole dataset (right side initially) (Implemented as an array. Index = label_id)\n",
    "        total_right = np.bincount(sorted_data[:, 1].astype(int))\n",
    "        total_left = np.zeros_like(total_right)\n",
    "        \n",
    "        best_split_index = -1\n",
    "        best_impurity = float('inf')\n",
    "\n",
    "        # Cumulative sums to track left and right distributions\n",
    "        for i in range(1, n):  # Iterate over sorted data (ignoring first since it's trivial)\n",
    "            label = int(sorted_data[i - 1, 1])\n",
    "            total_left[label] += 1  # Add this label to the left side\n",
    "            total_right[label] -= 1  # Remove this label from the right side\n",
    "            \n",
    "            l_count = i  # Left side count\n",
    "            r_count = n - i  # Right side count\n",
    "            \n",
    "            # Skip invalid splits where all data is on one side\n",
    "            if l_count == 0 or r_count == 0:\n",
    "                continue\n",
    "            \n",
    "            # Only evaluate splits when the feature value changes\n",
    "            if sorted_data[i - 1, 0] != sorted_data[i, 0]:\n",
    "                \n",
    "                if impur == \"gini\":\n",
    "                    impurity_l = tree.gini_impurity(total_left, l_count)\n",
    "                    impurity_r = tree.gini_impurity(total_right, r_count)\n",
    "                \n",
    "                if impur == \"entropy\":\n",
    "                    impurity_l = tree.binary_entropy(total_left, l_count)\n",
    "                    impurity_r = tree.binary_entropy(total_right, r_count)\n",
    "                    \n",
    "                \n",
    "                # Weighted average of left and right impurities\n",
    "                weighted_impurity = (l_count / n * impurity_l) + (r_count / n * impurity_r)\n",
    "                \n",
    "                # Track the best split\n",
    "                if weighted_impurity < best_impurity:\n",
    "                    best_impurity = weighted_impurity\n",
    "                    best_split_index = i\n",
    "        \n",
    "        # Return the feature value that defines the best split\n",
    "        if best_split_index != -1:\n",
    "            split_value = (sorted_data[best_split_index - 1, 0] + sorted_data[best_split_index, 0]) / 2\n",
    "            return split_value, best_impurity\n",
    "        else:\n",
    "            return None, None  # No valid split found\n",
    "\n",
    "    # Find the best split across all variables (features)\n",
    "    @staticmethod\n",
    "    def find_best_split(X, y, impur = \"gini\"):\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        best_impurity = float('inf')\n",
    "\n",
    "        # Combine X and y into one array for convenience\n",
    "        data = np.hstack((X, y[:, np.newaxis]))\n",
    "\n",
    "        # Iterate over all features (columns in X)\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            feature_data = data[:, [feature_idx, -1]]  # Select the feature and the labels\n",
    "            split_value, impurity = tree.split_single_variable_optimized(feature_data, impur)\n",
    "            \n",
    "            # Track the best split across all features\n",
    "            if impurity is not None and impurity < best_impurity:\n",
    "                best_impurity = impurity\n",
    "                best_feature = feature_idx\n",
    "                best_value = split_value\n",
    "        \n",
    "        return best_feature, best_value, best_impurity\n",
    "\n",
    "\n",
    "    def fit(self, X, y, max_depth=7, depth = 0, impur = \"gini\"): #stole the depth tracker from Guilia hehe\n",
    "\n",
    "        \n",
    "        split = self.find_best_split(X, y, impur=\"gini\")                \n",
    "        if depth == max_depth or len(set(y)) == 1 or split[0] is None:\n",
    "            p_class = np.bincount(y.astype(int)).argmax()\n",
    "            return self.node(prediction=p_class)\n",
    "\n",
    "        \n",
    "\n",
    "        if split[0] != None:\n",
    "            node = self.node(threshold = split[1], feature = split[0])\n",
    "    \n",
    "            Xy = np.hstack((X, y[:, np.newaxis]))\n",
    "            Xy_left = Xy[(Xy[:, node.feature] <= node.threshold)]\n",
    "            Xy_right = Xy[(Xy[:, node.feature] > node.threshold)]\n",
    "\n",
    "            X_left = Xy_left[:, :-1]\n",
    "            y_left = Xy_left[:, -1]\n",
    "\n",
    "            X_right = Xy_right[:, :-1]\n",
    "            y_right = Xy_right[:, -1]\n",
    "\n",
    "            if self.root is None:\n",
    "                self.root = node \n",
    "            \n",
    "            if X_left.size > 0 and y_left.size > 0:\n",
    "                node.left = self.fit(X_left, y_left, max_depth, depth=depth+1)\n",
    "            else:\n",
    "                p_class = np.bincount(y_right.astype(int)).argmax()\n",
    "                return self.node(prediction=p_class)\n",
    "            if X_right.size > 0 and y_right.size > 0:\n",
    "                node.right = self.fit(X_right, y_right, max_depth, depth=depth+1)\n",
    "            else:\n",
    "                p_class = np.bincount(y_left.astype(int)).argmax()\n",
    "                return self.node(prediction=p_class)\n",
    "              \n",
    "        return node\n",
    "        \n",
    "\n",
    "    def singlepredict(self, X, root=None):\n",
    "        if root is None:\n",
    "            root = self.root\n",
    "\n",
    "        if root.prediction is not None:\n",
    "            return root.prediction\n",
    "        \n",
    "        if X[root.feature] < root.threshold:\n",
    "            return self.singlepredict(X, root.left)\n",
    "        else:\n",
    "            return self.singlepredict(X, root.right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train[:, :-1]\n",
    "y = data_train[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_23716\\1944499316.py:82: RuntimeWarning: overflow encountered in scalar add\n",
      "  split_value = (sorted_data[best_split_index - 1, 0] + sorted_data[best_split_index, 0]) / 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.tree.node at 0x21950137390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TREE = tree()\n",
    "TREE.fit(X, y, 7, impur = \"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(int(len(data_test))):\n",
    "    xtest = data_test[i, :-1]\n",
    "    predictions.append(TREE.singlepredict(xtest))\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7518, array([0, 1, 2, 3, 4], dtype=int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(data_test[:,-1], predictions)\n",
    "accuracy, np.unique(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
